import streamlit as st
import cv2
import mediapipe as mp
import numpy as np
import joblib
import time
from collections import deque

# Cáº¥u hÃ¬nh trang Streamlit
st.set_page_config(layout="wide")

# ==============================
# Cáº¤U HÃŒNH & HÃ€M TIá»†N ÃCH
# ==============================
# CÃ¡c háº±ng sá»‘ Ä‘Æ°á»£c sá»­ dá»¥ng trong script gá»‘c
MODEL_PATH = "softmax_model_best.pkl"
SCALER_PATH = "scale.pkl"
SMOOTH_WINDOW = 8
CONF_THRESHOLD = 0.5
FPS_SMOOTH = 0.9
EXPECTED_FEATURE_SIZE = 9
EPS = 1e-6

# ==============================
# CACHING CÃC TÃ€I NGUYÃŠN Náº¶NG (MÃ´ hÃ¬nh vÃ  Mediapipe)
# ==============================
@st.cache_resource
def load_model_and_scaler():
    """Táº£i mÃ´ hÃ¬nh Softmax vÃ  dá»¯ liá»‡u chuáº©n hÃ³a chá»‰ má»™t láº§n."""
    try:
        # Load Model
        model_data = joblib.load(MODEL_PATH)
        W = model_data["W"]
        b = model_data["b"]
        CLASSES = model_data["classes"]
        idx2label = {i: lbl for i, lbl in enumerate(CLASSES)}

        # Load Scaler
        scaler_data = joblib.load(SCALER_PATH)
        X_mean = scaler_data["X_mean"]
        X_std = scaler_data["X_std"]
        
        # Kiá»ƒm tra kÃ­ch thÆ°á»›c Ä‘áº·c trÆ°ng
        if len(X_mean) != EXPECTED_FEATURE_SIZE:
            st.error(f"Lá»—i: KÃ­ch thÆ°á»›c Ä‘áº·c trÆ°ng cá»§a scaler ({len(X_mean)}) khÃ´ng khá»›p vá»›i ká»³ vá»ng ({EXPECTED_FEATURE_SIZE}).")
            return None, None, None, None, None, None
            
        st.success(f"âœ… ÄÃ£ load Model Softmax. CÃ¡c tráº¡ng thÃ¡i: {CLASSES}")
        return W, b, CLASSES, X_mean, X_std, idx2label
    except FileNotFoundError:
        st.error(f"Lá»—i: KhÃ´ng tÃ¬m tháº¥y file mÃ´ hÃ¬nh '{MODEL_PATH}' hoáº·c '{SCALER_PATH}'. HÃ£y Ä‘áº£m báº£o cÃ¡c file nÃ y náº±m trong cÃ¹ng thÆ° má»¥c vá»›i app.py.")
        return None, None, None, None, None, None
    except Exception as e:
        st.error(f"Lá»—i khi táº£i tÃ i nguyÃªn: {e}")
        return None, None, None, None, None, None

@st.cache_resource
def get_face_mesh_object():
    """Táº£i vÃ  cache Ä‘á»‘i tÆ°á»£ng MediaPipe FaceMesh chá»‰ má»™t láº§n."""
    mp_face_mesh = mp.solutions.face_mesh
    face_mesh = mp_face_mesh.FaceMesh(
        max_num_faces=1,
        refine_landmarks=True,
        min_detection_confidence=0.5,
        min_tracking_confidence=0.5
    )
    # Drawing Spec Ä‘Æ°á»£c dÃ¹ng náº¿u muá»‘n váº½ landmarks
    drawing_spec = mp_face_mesh.DrawingSpec(thickness=1, circle_radius=1)
    return face_mesh, drawing_spec

# ==============================
# KHAI BÃO CÃC CHá»ˆ Sá» LANDMARK
# ==============================
EYE_LEFT_IDX = np.array([33, 159, 145, 133, 153, 144])
EYE_RIGHT_IDX = np.array([362, 386, 374, 263, 380, 385])
MOUTH_IDX = np.array([61, 291, 0, 17, 78, 308])

# ==============================
# HÃ€M Dá»° ÄOÃN & TÃNH Äáº¶C TRÆ¯NG
# ==============================
def softmax(z):
    z = z - np.max(z)
    exp_z = np.exp(z)
    return exp_z / np.sum(exp_z)

def predict_proba(x, W, b):
    # Äáº£m báº£o x lÃ  máº£ng 2D (1, feature_size)
    if x.ndim == 1:
        x = x.reshape(1, -1)
    z = np.dot(x, W) + b
    return softmax(z)

def eye_aspect_ratio(landmarks, left=True):
    idx = EYE_LEFT_IDX if left else EYE_RIGHT_IDX
    pts = landmarks[idx, :2]
    A = np.linalg.norm(pts[1] - pts[5])
    B = np.linalg.norm(pts[2] - pts[4])
    C = np.linalg.norm(pts[0] - pts[3])
    return (A + B) / (2.0 * (C + EPS))

def mouth_aspect_ratio(landmarks):
    pts = landmarks[MOUTH_IDX, :2]
    A = np.linalg.norm(pts[0] - pts[1])
    B = np.linalg.norm(pts[4] - pts[5])
    C = np.linalg.norm(pts[2] - pts[3])
    return (A + B) / (2.0 * (C + EPS))

def head_pose_yaw_pitch_roll(landmarks):
    left_eye = landmarks[33][:2]
    right_eye = landmarks[263][:2]
    nose = landmarks[1][:2]
    chin = landmarks[152][:2]
    dx = right_eye[0] - left_eye[0]
    dy = right_eye[1] - left_eye[1]
    roll = np.degrees(np.arctan2(dy, dx + EPS))
    interocular = np.linalg.norm(right_eye - left_eye) + EPS
    eyes_center = (left_eye + right_eye) / 2.0
    yaw = np.degrees(np.arctan2((nose[0] - eyes_center[0]), interocular))
    baseline = chin - eyes_center
    pitch = np.degrees(np.arctan2((nose[1] - eyes_center[1]), (np.linalg.norm(baseline) + EPS)))
    return yaw, pitch, roll


# ==============================
# HÃ€M CHÃNH Cá»¦A STREAMLIT
# ==============================
def main():
    st.title("ðŸ‘¨â€ðŸ’» Demo GiÃ¡m sÃ¡t TÃ i xáº¿ (DMS) - Streamlit Softmax")
    st.markdown("""
    á»¨ng dá»¥ng nÃ y sá»­ dá»¥ng **MediaPipe Face Mesh** vÃ  mÃ´ hÃ¬nh **Softmax Regression** Ä‘á»ƒ dá»± Ä‘oÃ¡n tráº¡ng thÃ¡i cá»§a tÃ i xáº¿.
    
    âš ï¸ **LÆ°u Ã½ quan trá»ng:** á»¨ng dá»¥ng nÃ y sá»­ dá»¥ng `cv2.VideoCapture(0)` vÃ  **chá»‰ hoáº¡t Ä‘á»™ng khi cháº¡y cá»¥c bá»™** (trÃªn mÃ¡y tÃ­nh cá»§a báº¡n) vÃ  cáº§n cÃ³ webcam. NÃ³ sáº½ KHÃ”NG cháº¡y trÃªn cÃ¡c dá»‹ch vá»¥ cloud nhÆ° Streamlit Community Cloud do háº¡n cháº¿ truy cáº­p camera.
    """)
    
    # Táº£i mÃ´ hÃ¬nh vÃ  Mediapipe (Chá»‰ cháº¡y 1 láº§n)
    W, b, CLASSES, X_mean, X_std, idx2label = load_model_and_scaler()
    face_mesh, _ = get_face_mesh_object()

    if W is None:
        return # Dá»«ng náº¿u load model tháº¥t báº¡i

    # 1. KHá»žI Táº O STATE
    if 'run' not in st.session_state:
        st.session_state.run = False
    if 'pred_queue' not in st.session_state:
        st.session_state.pred_queue = deque(maxlen=SMOOTH_WINDOW)
    if 'pTime' not in st.session_state:
        st.session_state.pTime = time.time()
    if 'fps' not in st.session_state:
        st.session_state.fps = 0

    # 2. KHU Vá»°C ÄIá»€U KHIá»‚N
    col1, col2 = st.columns([1, 4])
    with col1:
        if st.session_state.run:
            stop_button = st.button("ðŸ”´ Dá»«ng Camera", key="stop", type="primary")
            if stop_button:
                st.session_state.run = False
                st.session_state.pred_queue = deque(maxlen=SMOOTH_WINDOW)
                # DÃ¹ng time.sleep ngáº¯n Ä‘á»ƒ Ä‘áº£m báº£o vÃ²ng láº·p káº¿t thÃºc
                time.sleep(0.1) 
                st.rerun()
        else:
            start_button = st.button("ðŸŸ¢ Báº¯t Ä‘áº§u Camera", key="start", type="primary")
            if start_button:
                st.session_state.run = True
                st.session_state.pTime = time.time()
                st.session_state.pred_queue = deque(maxlen=SMOOTH_WINDOW)
                st.rerun()

    # 3. VÃ’NG Láº¶P VIDEO VÃ€ HIá»‚N THá»Š
    
    # st.empty() táº¡o má»™t placeholder Ä‘á»ƒ cáº­p nháº­t liÃªn tá»¥c frame video
    st_frame = st.empty()
    st_status = st.empty()
    
    if st.session_state.run:
        # Má»Ÿ camera
        cap = cv2.VideoCapture(0)
        
        if not cap.isOpened():
            st_frame.error("âŒ KhÃ´ng thá»ƒ truy cáº­p Webcam. HÃ£y Ä‘áº£m báº£o webcam khÃ´ng Ä‘Æ°á»£c sá»­ dá»¥ng bá»Ÿi á»©ng dá»¥ng khÃ¡c vÃ  Ä‘Ã£ cáº¥p quyá»n.")
            st.session_state.run = False
            return
            
        st_status.info("â–¶ï¸ Äang cháº¡y. Nháº¥n 'Dá»«ng Camera' Ä‘á»ƒ thoÃ¡t.")

        # Láº¥y láº¡i cÃ¡c biáº¿n state
        pred_queue = st.session_state.pred_queue
        pTime = st.session_state.pTime
        fps = st.session_state.fps
        
        # VÃ²ng láº·p xá»­ lÃ½ frame
        while st.session_state.run:
            ret, frame = cap.read()
            if not ret:
                st_frame.error("Lá»—i Ä‘á»c khung hÃ¬nh tá»« Camera.")
                st.session_state.run = False
                break

            # Xá»­ lÃ½ frame
            frame = cv2.flip(frame, 1)
            h, w = frame.shape[:2]
            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = face_mesh.process(rgb)

            current_pred_label = "unknown"
            current_pred_conf = 0.0
            
            color_bgr = (0, 0, 255) # Máº·c Ä‘á»‹nh BGR: Äá»

            if results.multi_face_landmarks:
                face = results.multi_face_landmarks[0]
                # Láº¥y tá»a Ä‘á»™ landmark, nhÃ¢n vá»›i kÃ­ch thÆ°á»›c frame
                landmarks = np.array([[p.x * w, p.y * h, p.z * w] for p in face.landmark])

                # 1. TRÃCH XUáº¤T 9 Äáº¶C TRÆ¯NG Tá»¨C THá»œI
                ear_l = eye_aspect_ratio(landmarks, True)
                ear_r = eye_aspect_ratio(landmarks, False)
                mar = mouth_aspect_ratio(landmarks)
                yaw, pitch, roll = head_pose_yaw_pitch_roll(landmarks)
                nose, chin = landmarks[1], landmarks[152]
                angle_pitch_extra = np.degrees(np.arctan2(chin[1] - nose[1], (chin[2] - nose[2]) + EPS))
                forehead_y = np.mean(landmarks[[10, 338, 297, 332, 284], 1])
                cheek_dist = np.linalg.norm(landmarks[50] - landmarks[280])

                feats = np.array([ear_l, ear_r, mar, yaw, pitch, roll,
                                  angle_pitch_extra, forehead_y, cheek_dist], dtype=np.float32)

                # 2. CHUáº¨N HÃ“A (Scaling)
                feats_scaled = (feats - X_mean[:9]) / (X_std[:9] + EPS)

                # 3. Dá»° ÄOÃN Softmax
                probs = predict_proba(feats_scaled, W, b)
                probs = np.array(probs).flatten()
                pred_idx = np.argmax(probs)
                current_pred_conf = float(probs[pred_idx])
                current_pred_label = idx2label[pred_idx]

                # 4. LÃ€M MÆ¯á»¢T Káº¾T QUáº¢ Dá»° ÄOÃN
                if current_pred_conf > CONF_THRESHOLD:
                    pred_queue.append(current_pred_label)
                else:
                    pred_queue.append("unknown")


            # ======== SMOOTH PREDICTION ========
            if len(pred_queue) > 0 and pred_queue.count("unknown") < len(pred_queue):
                # Lá»c bá» "unknown" trÆ°á»›c khi tÃ¬m nhÃ£n phá»• biáº¿n nháº¥t
                valid_preds = [p for p in pred_queue if p != "unknown"]
                if valid_preds:
                    final_label = max(set(valid_preds), key=valid_preds.count)
                else:
                    final_label = "unknown"
            else:
                final_label = "unknown"

            # XÃ¡c Ä‘á»‹nh mÃ u hiá»ƒn thá»‹ (BGR)
            label_lower = final_label.lower()
            if "drowsy" in label_lower or "sleep" in label_lower:
                color_bgr = (0, 0, 255)       # Äá»
            elif "distract" in label_lower:
                color_bgr = (0, 255, 255)     # VÃ ng
            elif "awake" in label_lower or "calm" in label_lower:
                color_bgr = (255, 0, 0)       # Xanh dÆ°Æ¡ng
            else:
                color_bgr = (255, 255, 255)   # Tráº¯ng


            # ======== HIá»‚N THá»Š FPS ========
            cTime = time.time()
            fps = FPS_SMOOTH * fps + (1 - FPS_SMOOTH) * (1 / (cTime - pTime + EPS))
            pTime = cTime

            # Váº½ chá»¯ lÃªn frame (OpenCV BGR)
            cv2.putText(frame, f"FPS: {int(fps)}", (10, 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            cv2.putText(frame, f"State: {final_label} ({current_pred_conf:.2f})", (10, 70),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, color_bgr, 3)

            # Cáº­p nháº­t state trÆ°á»›c khi Streamlit reruns
            st.session_state.pTime = pTime
            st.session_state.fps = fps
            st.session_state.pred_queue = pred_queue

            # HIá»‚N THá»Š FRAME TRONG STREAMLIT
            # Chuyá»ƒn tá»« BGR (OpenCV) sang RGB (Streamlit)
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            st_frame.image(frame_rgb, channels="RGB", use_column_width=True)
        
        # Káº¿t thÃºc vÃ²ng láº·p
        cap.release()
        st_status.warning("ðŸ›‘ Camera Ä‘Ã£ dá»«ng.")

# Cháº¡y hÃ m chÃ­nh
if __name__ == '__main__':
    main()
