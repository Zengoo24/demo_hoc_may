import streamlit as st
import cv2
import mediapipe as mp
import numpy as np
import json
import av
from streamlit_webrtc import webrtc_streamer, VideoProcessorBase, WebRtcMode
import joblib
from collections import deque
import os
import warnings 
from PIL import Image

# Th√™m khai b√°o mp_drawing (MP Solutions Drawing Utilities)
mp_drawing = mp.solutions.drawing_utils

# ======================================================================
# I. C·∫§U H√åNH V√Ä H·∫∞NG S·ªê CHUNG
# ======================================================================

# --- C·∫•u h√¨nh chung ---
EPS = 1e-8 
NEW_WIDTH, NEW_HEIGHT = 640, 480 

# --- C·∫•u h√¨nh Drowsiness (Face Mesh) ---
MODEL_PATH = "softmax_model_best1.pkl"
SCALER_PATH = "scale1.pkl"
LABEL_MAP_PATH = "label_map_5cls.json"
SMOOTH_WINDOW = 5 
BLINK_THRESHOLD = 0.20 
N_FEATURES = 10 # S·ªë l∆∞·ª£ng ƒë·∫∑c tr∆∞ng mong ƒë·ª£i

# --- C·∫•u h√¨nh Wheel (Hands) ---
WHEEL_MODEL_PATH = "softmax_wheel_model.pkl"
WHEEL_SCALER_PATH = "scaler_wheel.pkl"


# ======================================================================
# II. C√ÅC H√ÄM T√çNH TO√ÅN C∆† B·∫¢N V√Ä T·∫¢I T√ÄI NGUY√äN
# ======================================================================

def softmax_predict(X, W, b):
    """Th·ª±c hi·ªán d·ª± ƒëo√°n Softmax (Face Mesh)."""
    logits = X @ W + b
    return np.argmax(logits, axis=1)

def softmax_wheel(z):
    """Th·ª±c hi·ªán d·ª± ƒëo√°n Softmax (Hands/Wheel)."""
    z -= np.max(z, axis=1, keepdims=True)
    exp_z = np.exp(z)
    return exp_z / np.sum(exp_z, axis=1, keepdims=True)

@st.cache_resource
def get_mp_hands_instance():
    """T·∫°o instance MediaPipe Hands (d√πng cho cache resource)."""
    return mp.solutions.hands.Hands(static_image_mode=True, max_num_hands=2)

@st.cache_resource
def load_assets():
    """T·∫£i t·∫•t c·∫£ tham s·ªë m√¥ h√¨nh, scaler v√† label map."""
    W_WHEEL, b_WHEEL, X_mean_WHEEL, X_std_WHEEL, CLASS_NAMES_WHEEL = [None] * 5
    
    try:
        # --- 1. T·∫£i M√¥ h√¨nh Face Mesh ---
        with open(MODEL_PATH, "rb") as f:
            model_data = joblib.load(f)
            W = model_data["W"]
            b = model_data["b"]
        with open(SCALER_PATH, "rb") as f:
            scaler_data = joblib.load(f)
            mean_data = scaler_data["X_mean"]
            std_data = scaler_data["X_std"]
        with open(LABEL_MAP_PATH, "r") as f:
            label_map = json.load(f)
            id2label = {int(v): k for k, v in label_map.items()}
            
        if W.shape[0] != N_FEATURES:
             st.error(f"L·ªñI KH√îNG T∆Ø∆†NG TH√çCH: M√¥ h√¨nh FACE MESH y√™u c·∫ßu {W.shape[0]} ƒë·∫∑c tr∆∞ng, nh∆∞ng ·ª©ng d·ª•ng n√†y tr√≠ch xu·∫•t {N_FEATURES} ƒë·∫∑c tr∆∞ng. Vui l√≤ng ki·ªÉm tra l·∫°i file model!")
             st.stop()
             
        # --- 2. T·∫£i M√¥ h√¨nh Wheel/Hands ---
        with open(WHEEL_MODEL_PATH, "rb") as f:
            wheel_model_data = joblib.load(f)
            W_WHEEL = wheel_model_data["W"]
            b_WHEEL = wheel_model_data["b"]
            CLASS_NAMES_WHEEL = wheel_model_data["classes"]
            
        with open(WHEEL_SCALER_PATH, "rb") as f:
            wheel_scaler_data = joblib.load(f)
            X_mean_WHEEL = wheel_scaler_data["X_mean"]
            X_std_WHEEL = wheel_scaler_data["X_std"]

        # ƒê√É S·ª¨A: B·ªè X_std_WHEEL b·ªã l·∫∑p th·ª´a
        return W, b, mean_data, std_data, id2label, W_WHEEL, b_WHEEL, X_mean_WHEEL, X_std_WHEEL, CLASS_NAMES_WHEEL

    except FileNotFoundError as e:
        # X·ª≠ l√Ω l·ªói n·∫øu thi·∫øu file model n√†o ƒë√≥
        st.error(f"L·ªñI FILE: Kh√¥ng t√¨m th·∫•y file t√†i nguy√™n V√î LƒÇNG ho·∫∑c KHU√îN M·∫∂T. Vui l√≤ng ki·ªÉm tra ƒë∆∞·ªùng d·∫´n: {e.filename}")
        st.stop()
    except Exception as e:
        st.error(f"L·ªñ·ªñI LOAD D·ªÆ LI·ªÜU: Chi ti·∫øt: {e}")
        st.stop()

# T·∫£i t√†i s·∫£n (Ch·∫°y m·ªôt l·∫ßn)
# ƒê√É S·ª¨A: B·ªè X_std_WHEEL b·ªã l·∫∑p th·ª´a, ch·ªâ g√°n 10 gi√° tr·ªã
W, b, mean, std, id2label, W_WHEEL, b_WHEEL, X_mean_WHEEL, X_std_WHEEL, CLASS_NAMES_WHEEL = load_assets()
classes = list(id2label.values())

# ======================================================================
# III. H√ÄM TR√çCH XU·∫§T ƒê·∫∂C TR∆ØNG KHU√îN M·∫∂T (FACE MESH)
# ======================================================================

mp_face_mesh = mp.solutions.face_mesh
EYE_LEFT_IDX = np.array([33, 159, 145, 133, 153, 144])
EYE_RIGHT_IDX = np.array([362, 386, 374, 263, 380, 385])
MOUTH_IDX = np.array([61, 291, 0, 17, 78, 308])

def eye_aspect_ratio(landmarks, left=True):
    idx = EYE_LEFT_IDX if left else EYE_RIGHT_IDX
    pts = landmarks[idx, :2]
    A = np.linalg.norm(pts[1] - pts[5])
    B = np.linalg.norm(pts[2] - pts[4])
    C = np.linalg.norm(pts[0] - pts[3])
    return (A + B) / (2.0 * (C + EPS))

def mouth_aspect_ratio(landmarks):
    pts = landmarks[MOUTH_IDX, :2]
    A = np.linalg.norm(pts[0] - pts[1])
    B = np.linalg.norm(pts[4] - pts[5])
    C = np.linalg.norm(pts[2] - pts[3])
    return (A + B) / (2.0 * (C + EPS))

def head_pose_yaw_pitch_roll(landmarks):
    left_eye = landmarks[33][:2]
    right_eye = landmarks[263][:2]
    nose = landmarks[1][:2]
    chin = landmarks[152][:2]

    dx = right_eye[0] - left_eye[0]
    dy = right_eye[1] - left_eye[1]
    roll = np.degrees(np.arctan2(dy, dx + EPS))

    interocular = np.linalg.norm(right_eye - left_eye) + EPS
    eyes_center = (left_eye + right_eye) / 2.0
    yaw = np.degrees(np.arctan2((nose[0] - eyes_center[0]), interocular))

    baseline = chin - eyes_center
    pitch = np.degrees(np.arctan2((nose[1] - eyes_center[1]), (np.linalg.norm(baseline) + EPS)))
    return yaw, pitch, roll

def get_extra_features(landmarks):
    nose, chin = landmarks[1], landmarks[152]
    angle_pitch_extra = np.degrees(np.arctan2(chin[1] - nose[1], (chin[2] - nose[2]) + EPS))
    forehead_y = np.mean(landmarks[[10, 338, 297, 332, 284], 1])
    cheek_dist = np.linalg.norm(landmarks[50] - landmarks[280])
    return angle_pitch_extra, forehead_y, cheek_dist

# ======================================================================
# IV. H√ÄM TR√çCH XU·∫§T ƒê·∫∂C TR∆ØNG V√î LƒÇNG (WHEEL/HANDS)
# ======================================================================

def detect_wheel_circle(frame):
    """S·ª≠ d·ª•ng Hough Transform ƒë·ªÉ ph√°t hi·ªán v√¥ lƒÉng."""
    # Frame ph·∫£i l√† BGR
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    gray = cv2.medianBlur(gray, 5)
    circles = cv2.HoughCircles(
        gray, cv2.HOUGH_GRADIENT,
        dp=1.0, minDist=120,
        param1=150, param2=40,
        minRadius=60, maxRadius=200
    )
    if circles is not None:
        circles = np.uint16(np.around(circles))
        x, y, r = circles[0, 0]
        return (x, y, r)
    return None

def extract_wheel_features(image, hands_processor, wheel):
    """Tr√≠ch xu·∫•t 128 ƒë·∫∑c tr∆∞ng tay v√† kho·∫£ng c√°ch c·ªï tay chu·∫©n h√≥a."""
    if wheel is None: return None
    xw, yw, rw = wheel
    h, w, _ = image.shape
    feats_all = []

    # Image ph·∫£i l√† RGB cho MediaPipe Hands
    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    res = hands_processor.process(rgb)
    if not res.multi_hand_landmarks: return None

    for hand_landmarks in res.multi_hand_landmarks:
        feats = []
        for lm in hand_landmarks.landmark:
            feats.extend([lm.x, lm.y, lm.z])

        # ƒê·∫∑c tr∆∞ng: Kho·∫£ng c√°ch c·ªï tay chu·∫©n h√≥a
        hx = hand_landmarks.landmark[0].x * w
        hy = hand_landmarks.landmark[0].y * h
        dist = np.sqrt((xw - hx) ** 2 + (yw - hy) ** 2)
        feats.append(dist / rw)

        feats_all.extend(feats)

    # ƒê·∫£m b·∫£o ƒë·ªß ƒë·ªô d√†i (128 = 64 * 2)
    feats_len_per_hand = 64
    expected_len = feats_len_per_hand * 2
    feats_all = feats_all[:expected_len]
    if len(feats_all) < expected_len:
        feats_all.extend([0.0] * (expected_len - len(feats_all)))

    return np.array(feats_all, dtype=np.float32)


# ======================================================================
# V. H√ÄM X·ª¨ L√ù ·∫¢NH Tƒ®NH V√Ä LIVE (Drowsiness)
# ======================================================================

def process_static_image(image_file, mesh, W, b, mean, std, id2label):
    # ƒê·ªçc ·∫£nh t·ª´ file uploader
    image = np.array(Image.open(image_file).convert('RGB'))
    
    # Resize ·∫£nh ƒë·ªÉ x·ª≠ l√Ω nhanh h∆°n v√† chu·∫©n h√≥a k√≠ch th∆∞·ªõc
    image_resized = cv2.resize(image, (NEW_WIDTH, NEW_HEIGHT))
    h, w = image_resized.shape[:2]
    
    # CHU·∫®N B·ªä ·∫¢NH CHO MEDIAPIPE (B·∫Øt bu·ªôc l·∫≠t ƒë·ªÉ t√≠nh landmarks ch√≠nh x√°c)
    image_for_mp = cv2.flip(image_resized, 1) # L·∫≠t ·∫£nh tr∆∞·ªõc khi x·ª≠ l√Ω
    
    # X·ª≠ l√Ω MediaPipe
    results = mesh.process(image_for_mp)
    
    result_label = "Ch∆∞a t√¨m th·∫•y khu√¥n m·∫∑t"
    
    # Chu·∫©n b·ªã ·∫£nh hi·ªÉn th·ªã: BGR v√† L·∫≠t l·∫°i ƒë·ªÉ ng∆∞·ªùi d√πng th·∫•y ·∫£nh ƒë√∫ng
    image_display_bgr = cv2.cvtColor(image_resized, cv2.COLOR_RGB2BGR)
    image_display_flipped = cv2.flip(image_display_bgr, 1)

    if results.multi_face_landmarks:
        landmarks = np.array([[p.x * w, p.y * h, p.z * w] for p in results.multi_face_landmarks[0].landmark])
        
        # 1. Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng
        ear_l = eye_aspect_ratio(landmarks, True)
        ear_r = eye_aspect_ratio(landmarks, False)
        ear_avg = (ear_l + ear_r) / 2.0
        mar = mouth_aspect_ratio(landmarks)
        yaw, pitch, roll = head_pose_yaw_pitch_roll(landmarks)
        angle_pitch_extra, forehead_y, cheek_dist = get_extra_features(landmarks)
        
        # 2. X·ª≠ l√Ω ƒë·∫∑c tr∆∞ng ƒë·ªông cho ·∫£nh tƒ©nh (DELTA_EAR = 0)
        delta_ear_value = 0.0 # B·∫±ng 0 v√¨ kh√¥ng c√≥ s·ª± thay ƒë·ªïi theo th·ªùi gian
        
        # 3. √ÅP D·ª§NG LU·∫¨T HEURISTIC C·ª®NG (∆Øu ti√™n BLINK n·∫øu m·∫Øt nh·∫Øm)
        if ear_avg < BLINK_THRESHOLD:
            result_label = "BLINK (Heuristic)"
        else:
            # 4. Ch·∫°y Softmax (10 ƒë·∫∑c tr∆∞ng)
            feats = np.array([ear_l, ear_r, mar, yaw, pitch, roll,
                              angle_pitch_extra, delta_ear_value, forehead_y, cheek_dist], dtype=np.float32)

            feats_scaled = (feats - mean[:N_FEATURES]) / (std[:N_FEATURES] + EPS)
            pred_idx = softmax_predict(np.expand_dims(feats_scaled, axis=0), W, b)[0]
            result_label = id2label.get(pred_idx, "UNKNOWN")
            
        # Hi·ªÉn th·ªã k·∫øt qu·∫£ l√™n ·∫£nh ƒë√£ l·∫≠t ng∆∞·ª£c l·∫°i (image_display_flipped)
        cv2.putText(image_display_flipped, f"Trang thai: {result_label.upper()}", (10, 70),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 3)

        # Chuy·ªÉn l·∫°i BGR sang RGB cho Streamlit
        final_image_rgb = cv2.cvtColor(image_display_flipped, cv2.COLOR_BGR2RGB)

        return final_image_rgb, result_label

    # Tr∆∞·ªùng h·ª£p kh√¥ng t√¨m th·∫•y khu√¥n m·∫∑t
    cv2.putText(image_display_flipped, "KHONG TIM THAY KHUON MAT", (10, h // 2),
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
    final_image_rgb = cv2.cvtColor(image_display_flipped, cv2.COLOR_BGR2RGB)
    
    return final_image_rgb, result_label

# ----------------------------------------------------------------------
## VI. H√ÄM X·ª¨ L√ù ·∫¢NH Tƒ®NH (Wheel)
# ----------------------------------------------------------------------
def process_static_wheel_image(image_file, W_WHEEL, b_WHEEL, X_mean_WHEEL, X_std_WHEEL, CLASS_NAMES_WHEEL):
    # ƒê·ªçc ·∫£nh t·ª´ file uploader
    img_pil = Image.open(image_file).convert('RGB')
    img_np = np.array(img_pil)
    
    # Convert RGB to BGR for OpenCV processing (HoughCircles)
    img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)
    
    # Khai b√°o mp_hands c·ª•c b·ªô
    mp_hands = mp.solutions.hands 
    hands_processor = get_mp_hands_instance()
    
    # 1. Ph√°t hi·ªán v√¥ lƒÉng
    wheel = detect_wheel_circle(img_bgr)
    
    if wheel is None:
        label = "KH√îNG T√åM TH·∫§Y V√î LƒÇNG"
        cv2.putText(img_bgr, label, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 2)
        return cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB), label

    # 2. Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng
    features = extract_wheel_features(img_bgr.copy(), hands_processor, wheel)
    
    img_display = img_bgr # B·∫Øt ƒë·∫ßu v·∫Ω tr√™n ·∫£nh BGR
    xw, yw, rw = wheel
    
    # Lu√¥n v·∫Ω v√¥ lƒÉng
    cv2.circle(img_display, (xw, yw), rw, (0, 255, 0), 2)
    cv2.circle(img_display, (xw, yw), 5, (0, 0, 255), -1)

    if features is None:
        label = "OFF-WHEEL (Tay kh√¥ng ƒë∆∞·ª£c ph√°t hi·ªán)"
        color = (0, 0, 255) # ƒê·ªè
        cv2.putText(img_display, label, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.0, color, 2)
        return cv2.cvtColor(img_display, cv2.COLOR_BGR2RGB), "OFF-WHEEL" # Tr·∫£ v·ªÅ nh√£n Off-wheel


    # 3. Chu·∫©n h√≥a v√† d·ª± ƒëo√°n
    X_sample = features.reshape(1, -1)
    X_scaled = (X_sample - X_mean_WHEEL) / (X_std_WHEEL + EPS) # S·ª≠ d·ª•ng X_mean_WHEEL, X_std_WHEEL

    z = X_scaled @ W_WHEEL + b_WHEEL # S·ª≠ d·ª•ng W_WHEEL, b_WHEEL
    probabilities = softmax_wheel(z)[0]

    predicted_index = np.argmax(probabilities)
    predicted_class = CLASS_NAMES_WHEEL[predicted_index]
    confidence = probabilities[predicted_index] * 100
    
    # --- Visualization (Tay) ---
    rgb_for_drawing = cv2.cvtColor(img_display, cv2.COLOR_BGR2RGB)
    res_for_drawing = hands_processor.process(rgb_for_drawing)
    
    if res_for_drawing.multi_hand_landmarks:
        for hand_landmarks in res_for_drawing.multi_hand_landmarks:
            # S·ª¨A L·ªñI: Thay th·∫ø mp_hands.drawing_utils b·∫±ng mp_drawing
            mp_drawing.draw_landmarks( 
                img_display, hand_landmarks, mp_hands.HAND_CONNECTIONS)

    # Hi·ªÉn th·ªã nh√£n d·ª± ƒëo√°n
    text = f"{predicted_class.upper()} ({confidence:.1f}%)"
    color = (0, 0, 255) if predicted_class == "off-wheel" else (0, 255, 0)
    
    # CƒÉn ch·ªânh text ƒë·ªÉ kh√¥ng tr√πng v·ªõi v√¥ lƒÉng
    cv2.putText(img_display, text, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.2, color, 3, cv2.LINE_AA)

    return cv2.cvtColor(img_display, cv2.COLOR_BGR2RGB), predicted_class.upper()


# ======================================================================
# VII. L·ªöP X·ª¨ L√ù VIDEO LIVE (WEBRTC PROCESSOR)
# ======================================================================
class DrowsinessProcessor(VideoProcessorBase):
    def __init__(self):
        # Kh·ªüi t·∫°o c√°c tham s·ªë v√† MediaPipe
        self.W = W; self.b = b; self.mean = mean; self.std = std; self.id2label = id2label
        self.face_mesh = mp_face_mesh.FaceMesh(
            max_num_faces=1, refine_landmarks=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)
        
        self.pred_queue = deque(maxlen=SMOOTH_WINDOW)
        self.last_pred_label = "CHO DU LIEU VAO"
        self.N_FEATURES = N_FEATURES
        self.last_ear_avg = 0.4 # L·ªãch s·ª≠ EAR cho Delta EAR

    def recv(self, frame: av.VideoFrame) -> av.VideoFrame:
        frame_array = frame.to_ndarray(format="bgr24")

        # 1. RESIZE FRAME
        frame_resized = cv2.resize(frame_array, (NEW_WIDTH, NEW_HEIGHT))
        h, w = frame_resized.shape[:2]

        rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)
        rgb_flipped = cv2.flip(rgb, 1) # L·∫≠t ·∫£nh ch·ªâ cho x·ª≠ l√Ω MediaPipe
        
        results = self.face_mesh.process(rgb_flipped)
        
        delta_ear_value = 0.0
        predicted_label_frame = "UNKNOWN"

        # --- 2. TR√çCH XU·∫§T 10 ƒê·∫∂C TR∆ØNG V√Ä D·ª∞ ƒêO√ÅN ---
        if results.multi_face_landmarks:
            landmarks = np.array([[p.x * w, p.y * h, p.z * w] for p in results.multi_face_landmarks[0].landmark])

            ear_l = eye_aspect_ratio(landmarks, True); ear_r = eye_aspect_ratio(landmarks, False); ear_avg = (ear_l + ear_r) / 2.0
            
            # 1. √ÅP D·ª§NG LU·∫¨T HEURISTIC C·ª®NG CHO BLINK 
            if ear_avg < BLINK_THRESHOLD:
                predicted_label_frame = "blink"
            else:
                # 2. S·ª¨ D·ª§NG SOFTMAX CHO C√ÅC H√ÄNH VI KH√ÅC
                
                mar = mouth_aspect_ratio(landmarks)
                yaw, pitch, roll = head_pose_yaw_pitch_roll(landmarks)
                angle_pitch_extra, forehead_y, cheek_dist = get_extra_features(landmarks)

                delta_ear_value = ear_avg - self.last_ear_avg 
                self.last_ear_avg = ear_avg

                feats = np.array([ear_l, ear_r, mar, yaw, pitch, roll,
                                angle_pitch_extra, delta_ear_value, forehead_y, cheek_dist], dtype=np.float32)

                feats_scaled = (feats - self.mean[:self.N_FEATURES]) / (self.std[:N_FEATURES] + EPS)
                pred_idx = softmax_predict(np.expand_dims(feats_scaled, axis=0), self.W, self.b)[0]
                predicted_label_frame = self.id2label.get(pred_idx, "UNKNOWN")
            
            self.pred_queue.append(predicted_label_frame)
        
        else:
             self.last_ear_avg = 0.4 

        # --- 4. SMOOTHING V√Ä HI·ªÇN TH·ªä K·∫æT QU·∫¢ ---
        if len(self.pred_queue) > 0:
            self.last_pred_label = max(set(self.pred_queue), key=self.pred_queue.count)
        
        cv2.putText(frame_resized, f"Trang thai: {self.last_pred_label.upper()}", (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 0), 3)
        cv2.putText(frame_resized, f"Delta EAR: {delta_ear_value:.3f}", (10, 110), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 255), 2)
        cv2.putText(frame_resized, f"EAR Threshold: <{BLINK_THRESHOLD}", (10, 150), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)

        return av.VideoFrame.from_ndarray(frame_resized, format="bgr24")

# ======================================================================
# VIII. GIAO DI·ªÜN STREAMLIT CH√çNH
# ======================================================================
st.set_page_config(page_title="Demo nh·∫≠n di·ªán c√°c h√†nh vi m·∫•t t·∫≠p trung - Softmax ", layout="wide")

tab1, tab2, tab3 = st.tabs(["üî¥ D·ª± ƒëo√°n Live Camera", "üñºÔ∏è D·ª± ƒëo√°n ·∫¢nh Tƒ©nh (Khu√¥n M·∫∑t)", "üöó Ki·ªÉm tra V√¥ LƒÉng (Tay)"])
mesh_static = mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, refine_landmarks=True)

with tab1:
    st.header("1. Nh·∫≠n di·ªán Tr·∫°ng th√°i Khu√¥n m·∫∑t (Live Camera)")
    st.warning("Vui l√≤ng ch·∫•p nh·∫≠n y√™u c·∫ßu truy c·∫≠p camera t·ª´ tr√¨nh duy·ªát c·ªßa b·∫°n.")
    st.markdown("---")

    col1, col2, col3 = st.columns([1, 4, 1]) 
    with col2: 
        webrtc_streamer(
            key="softmax_driver_live",
            mode=WebRtcMode.SENDRECV,
            rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]},
            video_processor_factory=DrowsinessProcessor,
            media_stream_constraints={"video": True, "audio": False},
            async_processing=True,
        )

with tab2:
    st.header("2. D·ª± ƒëo√°n ·∫¢nh Tƒ©nh (Khu√¥n M·∫∑t)")
    st.markdown("### T·∫£i l√™n ·∫£nh khu√¥n m·∫∑t ƒë·ªÉ d·ª± ƒëo√°n tr·∫°ng th√°i (Ng·ªß g·∫≠t/M·∫•t t·∫≠p trung)")
    uploaded_file = st.file_uploader("Ch·ªçn m·ªôt ·∫£nh khu√¥n m·∫∑t (.jpg, .png)", type=["jpg", "png", "jpeg"], key="face_upload")

    if uploaded_file is not None:
        st.info("ƒêang x·ª≠ l√Ω ·∫£nh... ")
        
        result_img_rgb, predicted_label = process_static_image(uploaded_file, mesh_static, W, b, mean, std, id2label)
        
        st.markdown("---")
        
        col_img, col_res = st.columns([2, 1])
        
        with col_img:
            st.image(result_img_rgb, caption="·∫¢nh ƒë√£ x·ª≠ l√Ω", use_column_width=True)
            
        with col_res:
            st.success("‚úÖ D·ª± ƒëo√°n Ho√†n t·∫•t")
            st.metric(label="Tr·∫°ng th√°i D·ª± ƒëo√°n", value=predicted_label.upper())
            st.caption(f"L∆∞u √Ω: Delta EAR cho ·∫£nh tƒ©nh lu√¥n b·∫±ng 0.")

    else:
        st.info("Vui l√≤ng t·∫£i l√™n m·ªôt ·∫£nh ƒë·ªÉ b·∫Øt ƒë·∫ßu d·ª± ƒëo√°n.")

with tab3:
    st.header("3. Ki·ªÉm tra V·ªã tr√≠ Tay (V√¥ LƒÉng)")
    st.warning(f"M√¥ h√¨nh V√¥ LƒÉng nh·∫≠n di·ªán: {CLASS_NAMES_WHEEL}")
    st.markdown("### T·∫£i l√™n ·∫£nh tay tr√™n/r·ªùi v√¥ lƒÉng ƒë·ªÉ d·ª± ƒëo√°n")
    uploaded_wheel_file = st.file_uploader("Ch·ªçn m·ªôt ·∫£nh v√¥ lƒÉng (.jpg, .png)", type=["jpg", "png", "jpeg"], key="wheel_upload")
    
    if uploaded_wheel_file is not None:
        st.info("ƒêang x·ª≠ l√Ω ·∫£nh...")
        
        # X·ª≠ l√Ω v√† d·ª± ƒëo√°n
        result_img_rgb, predicted_label = process_static_wheel_image(uploaded_wheel_file, W_WHEEL, b_WHEEL, X_mean_WHEEL, X_std_WHEEL, CLASS_NAMES_WHEEL)
        
        st.markdown("---")
        
        col_img, col_res = st.columns([2, 1])

        with col_img:
            st.image(result_img_rgb, caption="·∫¢nh ƒë√£ x·ª≠ l√Ω (V√¥ lƒÉng, Tay)", use_column_width=True)
            
        with col_res:
            st.success("‚úÖ D·ª± ƒëo√°n Ho√†n t·∫•t")
            st.metric(label="V·ªã tr√≠ Tay D·ª± ƒëo√°n", value=predicted_label.upper())
            st.caption("Ki·ªÉm tra m√†u s·∫Øc: Xanh l√° (On-wheel), ƒê·ªè (Off-wheel)")
            
    else:
        st.info("Vui l√≤ng t·∫£i l√™n m·ªôt ·∫£nh l√°i xe ƒë·ªÉ ki·ªÉm tra v·ªã tr√≠ tay.")

